{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datas header =  ['色泽', '根蒂', '敲声', '纹理', '脐部', '触感', '好瓜']\n",
      "[['青绿' '蜷缩' '浊响' '清晰' '凹陷' '硬滑']\n",
      " ['乌黑' '蜷缩' '沉闷' '清晰' '凹陷' '硬滑']\n",
      " ['乌黑' '蜷缩' '浊响' '清晰' '凹陷' '硬滑']\n",
      " ['青绿' '蜷缩' '沉闷' '清晰' '凹陷' '硬滑']\n",
      " ['浅白' '蜷缩' '浊响' '清晰' '凹陷' '硬滑']\n",
      " ['青绿' '稍蜷' '浊响' '清晰' '稍凹' '软粘']\n",
      " ['乌黑' '稍蜷' '浊响' '稍糊' '稍凹' '软粘']\n",
      " ['乌黑' '稍蜷' '浊响' '清晰' '稍凹' '硬滑']\n",
      " ['乌黑' '稍蜷' '沉闷' '稍糊' '稍凹' '硬滑']\n",
      " ['青绿' '硬挺' '清脆' '清晰' '平坦' '软粘']\n",
      " ['浅白' '硬挺' '清脆' '模糊' '平坦' '硬滑']\n",
      " ['浅白' '蜷缩' '浊响' '模糊' '平坦' '软粘']\n",
      " ['青绿' '稍蜷' '浊响' '稍糊' '凹陷' '硬滑']\n",
      " ['浅白' '稍蜷' '沉闷' '稍糊' '凹陷' '硬滑']\n",
      " ['乌黑' '稍蜷' '浊响' '清晰' '稍凹' '软粘']\n",
      " ['浅白' '蜷缩' '浊响' '模糊' '平坦' '硬滑']\n",
      " ['青绿' '蜷缩' '沉闷' '稍糊' '稍凹' '硬滑']]\n",
      "Labels =  ['是' '是' '是' '是' '是' '是' '是' '是' '否' '否' '否' '否' '否' '否' '否' '否' '否']\n",
      "split\n",
      "[['青绿' '稍蜷' '浊响' '清晰' '稍凹' '软粘']\n",
      " ['浅白' '蜷缩' '浊响' '模糊' '平坦' '硬滑']\n",
      " ['浅白' '蜷缩' '浊响' '模糊' '平坦' '软粘']\n",
      " ['乌黑' '稍蜷' '浊响' '清晰' '稍凹' '软粘']\n",
      " ['乌黑' '稍蜷' '沉闷' '稍糊' '稍凹' '硬滑']\n",
      " ['浅白' '稍蜷' '沉闷' '稍糊' '凹陷' '硬滑']\n",
      " ['乌黑' '蜷缩' '浊响' '清晰' '凹陷' '硬滑']\n",
      " ['青绿' '硬挺' '清脆' '清晰' '平坦' '软粘']\n",
      " ['青绿' '蜷缩' '沉闷' '稍糊' '稍凹' '硬滑']\n",
      " ['浅白' '蜷缩' '浊响' '清晰' '凹陷' '硬滑']\n",
      " ['乌黑' '稍蜷' '浊响' '清晰' '稍凹' '硬滑']\n",
      " ['浅白' '硬挺' '清脆' '模糊' '平坦' '硬滑']\n",
      " ['青绿' '稍蜷' '浊响' '稍糊' '凹陷' '硬滑']\n",
      " ['青绿' '蜷缩' '沉闷' '清晰' '凹陷' '硬滑']\n",
      " ['乌黑' '稍蜷' '浊响' '稍糊' '稍凹' '软粘']]\n",
      "split\n",
      "[['青绿' '蜷缩' '浊响' '清晰' '凹陷' '硬滑']\n",
      " ['乌黑' '蜷缩' '沉闷' '清晰' '凹陷' '硬滑']]\n",
      "split\n",
      "['是' '否' '否' '否' '否' '否' '是' '否' '否' '是' '是' '否' '否' '是' '是']\n",
      "split\n",
      "['是' '是']\n"
     ]
    }
   ],
   "source": [
    "#周志华西瓜数据集：根据特征是否好瓜\n",
    "txt_path=r\".\\Watermelon.txt\"\n",
    "with open(txt_path, encoding='utf-8') as t:\n",
    "     result=[]\n",
    "     lines = t.read().splitlines()\n",
    "     for i in range(len(lines)):\n",
    "        line=lines[i].split(',')\n",
    "        del line[0]\n",
    "        result.append(line)\n",
    "array = np.asarray(result)\n",
    "data_header = array[0,:].tolist()\n",
    "print('Datas header = ',data_header)\n",
    "data = array[1:,:-1] \n",
    "print(data)\n",
    "labels = array[1:,-1] \n",
    "print('Labels = ', labels)\n",
    "X_train = data\n",
    "y_train = labels\n",
    "#分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.1, random_state=42)\n",
    "print(\"split\")\n",
    "print(X_train)\n",
    "print(\"split\")\n",
    "print(X_test)\n",
    "print(\"split\")\n",
    "print(y_train)\n",
    "print(\"split\")\n",
    "print(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建决策树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shannon Entropy: 0.9709505944546686\n"
     ]
    }
   ],
   "source": [
    "def get_shannon_entropy(labels: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    输入：labels：节点中所有数据的标签，用于计算概率\n",
    "    输出：shannon_entropy：香农熵\n",
    "    \"\"\"\n",
    "    #统计并用字典存储节点中不同标签的所含样本的个数\n",
    "    label_counts = Counter(labels)  # Count occurrences of each label\n",
    "    total_samples = len(labels)  # Total number of samples\n",
    "\n",
    "    shannon_entropy = 0.0\n",
    "    #根据字典，计算概率和香农熵\n",
    "    for label_count in label_counts.values():\n",
    "        # Calculate the probability of each label\n",
    "        probability = label_count / total_samples\n",
    "        # Calculate entropy for each label and sum them up\n",
    "        shannon_entropy -= probability * math.log2(probability)\n",
    "    return shannon_entropy\n",
    "\n",
    "# Example usage:\n",
    "labels = np.array(['是', '是', '否', '否', '否'])\n",
    "entropy = get_shannon_entropy(labels)\n",
    "print(\"Shannon Entropy:\", entropy)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditional Entropy: 0.4\n"
     ]
    }
   ],
   "source": [
    "def get_conditional_entropy(datas: np.ndarray, labels: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    输入：datas:具体来说会是某一维特征的数据； labels：所有数据的标签用于计算概率\n",
    "    输出：conditional_entropy：根据某一维特征划分的条件熵\n",
    "    \"\"\"\n",
    "    \n",
    "    #统计并存储该维特征所具有的值对应的样本数\n",
    "    total_samples = len(labels)  # Total number of samples\n",
    "    conditional_entropy = 0.0\n",
    "\n",
    "    #计算条件熵（考虑条件熵和香农熵的关联）\n",
    "    # Count occurrences of each unique value in datas\n",
    "    value_counts = Counter(datas)\n",
    "\n",
    "    for value, count in value_counts.items():\n",
    "        # Calculate probability of the specific value\n",
    "        probability = count / total_samples\n",
    "\n",
    "        # Filter labels where datas match the specific value\n",
    "        subset_labels = labels[datas == value]\n",
    "\n",
    "        # Calculate the Shannon entropy for this subset\n",
    "        subset_entropy = get_shannon_entropy(subset_labels)\n",
    "\n",
    "        # Weighted conditional entropy\n",
    "        conditional_entropy += probability * subset_entropy\n",
    "\n",
    "    \n",
    "    return conditional_entropy\n",
    "\n",
    "# Example usage:\n",
    "datas = np.array(['青绿', '乌黑', '乌黑', '青绿', '浅白'])\n",
    "labels = np.array(['是', '是', '是', '否', '否'])\n",
    "\n",
    "conditional_entropy = get_conditional_entropy(datas, labels)\n",
    "print(\"Conditional Entropy:\", conditional_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_best_gain(datas: np.ndarray, labels: np.ndarray) -> (float, float):\n",
    "    \"\"\"\n",
    "    对比根据每一维特征求得的增益Gain，选出最佳特征\n",
    "    输入：datas：整个数据集；labels：所有数据的标签\n",
    "    输出：best_feature：当前最佳特征； best_gain：当前最佳增益\n",
    "    \"\"\"\n",
    "    #思考信息增益的计算公式\n",
    "    total_entropy = get_shannon_entropy(labels)\n",
    "    best_feature = None\n",
    "    best_gain = -1.0\n",
    "\n",
    "    \n",
    "    #分别对每个特征计算信息增益，并选出增益最大的特征\n",
    "\n",
    "    for feature_index in range(datas.shape[1]):\n",
    "        feature_values = datas[:, feature_index]\n",
    "        unique_values = np.unique(feature_values)\n",
    "\n",
    "        conditional_entropy = 0.0\n",
    "        for value in unique_values:\n",
    "            subset_indices = np.where(feature_values == value)[0]\n",
    "            subset_labels = labels[subset_indices]\n",
    "            probability = len(subset_indices) / len(labels)\n",
    "            conditional_entropy += probability * get_shannon_entropy(subset_labels)\n",
    "\n",
    "        information_gain = total_entropy - conditional_entropy\n",
    "        if information_gain > best_gain:\n",
    "            best_gain = information_gain\n",
    "            best_feature = data_header[feature_index]\n",
    "    print(\"Best Feature:\", best_feature)\n",
    "    print(\"Best Gain:\", best_gain)\n",
    "    return best_feature, best_gain\n",
    "\n",
    "# Example usage:\n",
    "#data = np.array([('青绿', '蜷缩', '浊响', '清晰', '凹陷', '硬滑'),\n",
    "#                 ('乌黑', '蜷缩', '沉闷', '清晰', '凹陷', '硬滑'),\n",
    "#                ('青绿', '蜷缩', '沉闷', '清晰', '凹陷', '硬滑')\n",
    "#                 ])\n",
    "\n",
    "# labels = np.array(['是', '否', '否'])\n",
    "\n",
    "# best_feature, best_gain = get_best_gain(data, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_tree(datas_header: list, datas: np.ndarray, labels: np.ndarray) -> dict:\n",
    "    \"\"\"\n",
    "    对比根据每一维特征求得的增益Gain，选出最佳特征\n",
    "    输入：datas：整个数据集；labels：所有数据的标签\n",
    "    输出：best_feature：当前最佳特征； best_gain：当前最佳增益\n",
    "    \"\"\"\n",
    "    # If all labels are the same, return a leaf node with that label\n",
    "    if len(set(labels)) == 1:\n",
    "        return labels[0]\n",
    "\n",
    "    # If there are no features left to split on, return the majority class\n",
    "    if len(datas_header) == 0:\n",
    "        label_counts = Counter(labels)\n",
    "        return label_counts.most_common(1)[0][0]\n",
    "\n",
    "    # Find the best feature to split on and its information gain\n",
    "    best_feature, _ = get_best_gain(datas, labels)\n",
    "\n",
    "    # Initialize the tree with the best_feature as the root\n",
    "    tree = {best_feature: {}}\n",
    "\n",
    "    # Remove the best_feature from the available features\n",
    "    # remaining_features = [f for f in datas_header if f != best_feature]\n",
    "    remaining_features = datas_header\n",
    "    \n",
    "    print(datas_header)\n",
    "    print(best_feature)\n",
    "    # Split the data into subsets based on unique values of the best_feature\n",
    "    unique_values = np.unique(datas[:, datas_header.index(best_feature)])\n",
    "\n",
    "\n",
    "    for value in unique_values:\n",
    "        subset_indices = np.where(datas[:, datas_header.index(best_feature)] == value)\n",
    "        subset_data = datas[subset_indices]\n",
    "        subset_labels = labels[subset_indices]\n",
    "\n",
    "        # Recursively create a subtree for each subset of data\n",
    "        subtree = create_tree(remaining_features, subset_data, subset_labels)\n",
    "        tree[best_feature][value] = subtree\n",
    "\n",
    "    #递归出口（结束条件）写在前面，判断是否继续产生叶子节点\n",
    "    \n",
    "    # 结束条件1：节点中所有数据都是同类，该节点不再产生叶子节点\n",
    "    \n",
    "    # 结束条件2: 节点中还有不同类的标签，而没有节点能产生叶子节点，少数服从多数\n",
    "    \n",
    "    #递归体：判断当前最优特征，划分叶子节点，并在叶子节点的划分中不再考虑当前最优特征\n",
    "    \n",
    "    return tree\n",
    "\n",
    "# data_header = ['色泽', '根蒂', '敲声', '纹理', '脐部', '触感', '好瓜']\n",
    "# datas = np.array([\n",
    "#    ('青绿', '蜷缩', '浊响', '清晰', '凹陷', '硬滑'),\n",
    "#    ('乌黑', '蜷缩', '沉闷', '清晰', '凹陷', '硬滑'),\n",
    "#    ('乌黑', '稍蜷', '浊响', '清晰', '稍凹', '硬滑'),\n",
    "#    ('青绿', '蜷缩', '沉闷', '清晰', '稍凹', '硬滑'),\n",
    "#    ('浅白', '蜷缩', '浊响', '清晰', '稍凹', '硬滑')\n",
    "# ])\n",
    "# labels = np.array(['是', '是', '是', '否', '否'])\n",
    "\n",
    "# Call the create_tree function\n",
    "# decision_tree = create_tree(data_header, data, labels)\n",
    "\n",
    "# Print the resulting decision tree\n",
    "# import json\n",
    "# print(json.dumps(decision_tree, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Predict_Results(tree_model, datas, datas_header):\n",
    "    \"\"\"\n",
    "    测试\n",
    "    \"\"\"\n",
    "    def predict_result(trees_model: dict, input_data: np, datas_header: list) -> str:\n",
    "        cur_judge = list(trees_model.keys())[0]     \n",
    "        num_feature = datas_header.index(cur_judge)     \n",
    "        cur_val = input_data[num_feature]       \n",
    "        cur_tree = trees_model[cur_judge]\n",
    "        if type(cur_tree[cur_val]) == np.str_:\n",
    "            return cur_tree[cur_val]\n",
    "        return predict_result(cur_tree[cur_val], input_data, datas_header)\n",
    "    \n",
    "    results = np.zeros(datas.shape[0]).astype(str)\n",
    "    for i,data in zip(range(datas.shape[0]), datas):\n",
    "        data = data.tolist()\n",
    "        result = predict_result(tree_model, data, datas_header)\n",
    "        results[i] = result\n",
    "    return results\n",
    "\n",
    "def evaluation(y, results):\n",
    "    num = y.shape[0]\n",
    "    acc = sum(y==results) / num\n",
    "    print('Accuracy = ', float(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存和读取函数\n",
    "def store_tree(input_tree, filename):\n",
    "    import pickle\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(input_tree, f)\n",
    "        f.close()\n",
    "\n",
    "\n",
    "def restore_tree(filename):\n",
    "    import pickle\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Feature: 纹理\n",
      "Best Gain: 0.32751829749445327\n",
      "['色泽', '根蒂', '敲声', '纹理', '脐部', '触感', '好瓜']\n",
      "纹理\n",
      "Best Feature: 根蒂\n",
      "Best Gain: 0.46956521111470695\n",
      "['色泽', '根蒂', '敲声', '纹理', '脐部', '触感', '好瓜']\n",
      "根蒂\n",
      "Best Feature: 色泽\n",
      "Best Gain: 0.2516291673878229\n",
      "['色泽', '根蒂', '敲声', '纹理', '脐部', '触感', '好瓜']\n",
      "色泽\n",
      "Best Feature: 触感\n",
      "Best Gain: 1.0\n",
      "['色泽', '根蒂', '敲声', '纹理', '脐部', '触感', '好瓜']\n",
      "触感\n",
      "Best Feature: 触感\n",
      "Best Gain: 0.7219280948873623\n",
      "['色泽', '根蒂', '敲声', '纹理', '脐部', '触感', '好瓜']\n",
      "触感\n",
      "决策树构建完成\n",
      "{'纹理': {'模糊': '否', '清晰': {'根蒂': {'硬挺': '否', '稍蜷': {'色泽': {'乌黑': {'触感': {'硬滑': '是', '软粘': '否'}}, '青绿': '是'}}, '蜷缩': '是'}}, '稍糊': {'触感': {'硬滑': '否', '软粘': '是'}}}}\n",
      "Accuracy =  1.0\n",
      "模型保存完毕\n"
     ]
    }
   ],
   "source": [
    "# 创建树\n",
    "data_header = ['色泽', '根蒂', '敲声', '纹理', '脐部', '触感','好瓜']\n",
    "tree_model = create_tree(data_header, X_train, y_train)\n",
    "print(\"决策树构建完成\")\n",
    "print(tree_model)\n",
    "\n",
    "results = Predict_Results(tree_model, X_test, data_header)\n",
    "evaluation(y_test,results)\n",
    "\n",
    "# # 保存树模型\n",
    "store_tree(tree_model, 'Watermelon.pkl')     \n",
    "print(\"模型保存完毕\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
