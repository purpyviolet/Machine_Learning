{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "逻辑斯蒂回归(LogisticRegression)是一个重要的分类模型，该算法将线性回归的输出概率化，根据“概率”进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pylab import scatter, show, legend, xlabel, ylabel\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据：学科成绩为特征的二分类问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   grade1     100 non-null    float64\n",
      " 1   grade2     100 non-null    float64\n",
      " 2   label;;;;  100 non-null    object \n",
      "dtypes: float64(2), object(1)\n",
      "memory usage: 2.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data.csv\", header=0)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[34.62365962 78.02469282]\n",
      " [30.28671077 43.89499752]\n",
      " [35.84740877 72.90219803]\n",
      " [60.18259939 86.3085521 ]\n",
      " [79.03273605 75.34437644]\n",
      " [45.08327748 56.31637178]\n",
      " [61.10666454 96.51142588]\n",
      " [75.02474557 46.55401354]\n",
      " [76.0987867  87.42056972]\n",
      " [84.43281996 43.53339331]\n",
      " [95.86155507 38.22527806]\n",
      " [75.01365839 30.60326323]\n",
      " [82.30705337 76.4819633 ]\n",
      " [69.36458876 97.71869196]\n",
      " [39.53833914 76.03681085]\n",
      " [53.97105215 89.20735014]\n",
      " [69.07014406 52.74046973]\n",
      " [67.94685548 46.67857411]\n",
      " [70.66150955 92.92713789]\n",
      " [76.97878373 47.57596365]\n",
      " [67.37202755 42.83843832]\n",
      " [89.67677575 65.79936593]\n",
      " [50.53478829 48.85581153]\n",
      " [34.21206098 44.2095286 ]\n",
      " [77.92409145 68.97235999]\n",
      " [62.27101367 69.95445795]\n",
      " [80.19018075 44.82162893]\n",
      " [93.1143888  38.80067034]\n",
      " [61.83020602 50.25610789]\n",
      " [38.7858038  64.99568096]\n",
      " [61.37928945 72.80788731]\n",
      " [85.40451939 57.05198398]\n",
      " [52.10797973 63.12762377]\n",
      " [52.04540477 69.43286012]\n",
      " [40.23689374 71.16774802]\n",
      " [54.63510555 52.21388588]\n",
      " [33.91550011 98.86943574]\n",
      " [64.17698887 80.90806059]\n",
      " [74.78925296 41.57341523]\n",
      " [34.18364003 75.23772034]\n",
      " [83.90239366 56.30804622]\n",
      " [51.54772027 46.85629026]\n",
      " [94.44336777 65.56892161]\n",
      " [82.36875376 40.61825516]\n",
      " [51.04775177 45.82270146]\n",
      " [62.22267576 52.06099195]\n",
      " [77.19303493 70.4582    ]\n",
      " [97.77159928 86.72782233]\n",
      " [62.0730638  96.76882412]\n",
      " [91.5649745  88.69629255]\n",
      " [79.94481794 74.16311935]\n",
      " [99.27252693 60.999031  ]\n",
      " [90.54671411 43.39060181]\n",
      " [34.52451385 60.39634246]\n",
      " [50.28649612 49.80453881]\n",
      " [49.58667722 59.80895099]\n",
      " [97.64563396 68.86157272]\n",
      " [32.57720017 95.59854761]\n",
      " [74.24869137 69.82457123]\n",
      " [71.79646206 78.45356225]\n",
      " [75.39561147 85.75993667]\n",
      " [35.28611282 47.02051395]\n",
      " [56.2538175  39.26147251]\n",
      " [30.05882245 49.59297387]\n",
      " [44.66826172 66.45008615]\n",
      " [66.56089447 41.09209808]\n",
      " [40.45755098 97.53518549]\n",
      " [49.07256322 51.88321182]\n",
      " [80.27957401 92.11606081]\n",
      " [66.74671857 60.99139403]\n",
      " [32.72283304 43.30717306]\n",
      " [64.03932042 78.03168802]\n",
      " [72.34649423 96.22759297]\n",
      " [60.45788574 73.0949981 ]\n",
      " [58.84095622 75.85844831]\n",
      " [99.8278578  72.36925193]\n",
      " [47.26426911 88.475865  ]\n",
      " [50.4581598  75.80985953]\n",
      " [60.45555629 42.50840944]\n",
      " [82.22666158 42.71987854]\n",
      " [88.91389642 69.8037889 ]\n",
      " [94.83450672 45.6943068 ]\n",
      " [67.31925747 66.58935318]\n",
      " [57.23870632 59.51428198]\n",
      " [80.366756   90.9601479 ]\n",
      " [68.46852179 85.5943071 ]\n",
      " [42.07545454 78.844786  ]\n",
      " [75.47770201 90.424539  ]\n",
      " [78.63542435 96.64742717]\n",
      " [52.34800399 60.76950526]\n",
      " [94.09433113 77.15910509]\n",
      " [90.44855097 87.50879176]\n",
      " [55.48216114 35.57070347]\n",
      " [74.49269242 84.84513685]\n",
      " [89.84580671 45.35828361]\n",
      " [83.48916274 48.3802858 ]\n",
      " [42.26170081 87.10385094]\n",
      " [99.31500881 68.77540947]\n",
      " [55.34001756 64.93193801]\n",
      " [74.775893   89.5298129 ]]\n",
      "[0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 1. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "df.columns = [\"grade1\",\"grade2\",\"label\"] \n",
    "#df的第三个列名有；；；\n",
    "x = df[\"label\"].map(lambda x: float(x.rstrip(';')))\n",
    "X = df[[\"grade1\",\"grade2\"]]\n",
    "X = np.array(X)\n",
    "print(X)\n",
    "#数据min_max归一化到（-1，1）间 \n",
    "min_max_scaler = preprocessing.MinMaxScaler(feature_range=(-1,1))\n",
    "X = min_max_scaler.fit_transform(X)\n",
    "#数据集label中有；；；；要去掉\n",
    "Y = df[\"label\"].map(lambda x: float(x.rstrip(';')))\n",
    "Y = np.array(Y)\n",
    "print(Y)\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 设定拟合函数（Hypothesis Function）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "逻辑斯蒂回归将线性回归拟合函数的输出通过Sigmoid函数映射到概率区间[0,1]，利用概率构造二分类模型（实际上是分类模型）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性回归函数： $z=\\theta^Tx$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid函数： $g(z) = \\frac{1}{1+e^{-z}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./sigmoid.png\" width=500 height=5000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "逻辑斯蒂拟合函数(Hypothesis Function)：$h_\\theta(x) = \\frac{1}{1+e^{-\\theta^Tx}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构造逻辑斯蒂拟合函数\n",
    "def Sigmoid(z):\n",
    "    \"\"\"\n",
    "    Sigmoid函数：输入为线性回归函数值\n",
    "    \"\"\"\n",
    "    sig_z = 1 / (1+ np.exp(-z))\n",
    "    return sig_z \n",
    "def Hypothesis(theta, x):\n",
    "    \"\"\"\n",
    "    逻辑斯蒂拟合函数：线性回归函数和Sigmoid函数的复合\n",
    "    输入：theta(线性回归学习权重，与数据的维度相同)、x(数据)\n",
    "    输出：Sigmoid(z)（拟合函数）(z为线性回归函数)\n",
    "    \"\"\"\n",
    "    z = np.dot(theta, x)\n",
    "    return Sigmoid(z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 设定代价函数（Cost Function）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性回归的代价函数（均方误差）对于sigmoid()是非凸的,容易使优化落入局部极小值。\n",
    "因此借鉴概率论中极大似然估计的思想，选用以下代价函数，可保证优化的效果："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./costfun1.png\" width=500 height=5000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该式可写为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./costfun2.png\" width=500 height=5000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出，这样的代价函数在y=1的前提下，如果预测值越接近1，那么相应代价就越小，反之则越大（对于y=0亦然）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设m为样本数："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./costfun3.png\" width=1000 height=1000>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cost_Function(X,Y,theta):\n",
    "    \"\"\"\n",
    "    代价函数：按标签对每个样本的“损失”进行累加并平均\n",
    "    输入：X（数据）、Y（标签）、theta(当前theta)\n",
    "    输出：J（损失函数值）\n",
    "    \"\"\"\n",
    "    J = 0\n",
    "    for i in (0,len(Y)-1):\n",
    "        z = Hypothesis(theta, X[i])\n",
    "        Z = Sigmoid(z) # 这个相当于是h_theta(x)\n",
    "        if Y[i] == 1:\n",
    "            J += -np.log(Z)\n",
    "        if Y[i] == 0:\n",
    "            J += -np.log(1 - Z)\n",
    "        J = J / len(X)\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对代价函数进行梯度下降优化Theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于Theta中的第j个权重$\\theta_j$,其一轮的梯度下降更新为：$\\theta_j^{'} = \\theta_j-\\frac{\\alpha}{m} \\sum_{i=1}^m(h_\\theta(x^{(i)}-y^{(i)})x_j^{(i)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gradient_Descent(X,Y,theta,alpha):\n",
    "    \"\"\"\n",
    "    一轮梯度下降更新theta\n",
    "    输入：X（数据）、Y（标签）、theta(当前theta，要被更新)、alpha（学习率）\n",
    "    输出：new_theta（更新的theta）\n",
    "    \"\"\"\n",
    "    m = len(Y)\n",
    "    new_theta = theta\n",
    "    for j in range(len(theta)):\n",
    "        sumErrors = 0\n",
    "        for i in range(m):\n",
    "            xi = X[i]\n",
    "            yi = Y[i]\n",
    "            h = Hypothesis(theta,X[i])\n",
    "            sumErrors += (h-yi)*xi[j]\n",
    "        new_theta[j] = theta[j] - alpha*1.0/m*sumErrors\n",
    "    \n",
    "#     m = len(Y)\n",
    "#     num_features = len(X[0])  # Number of features\n",
    "#     new_theta = np.copy(theta)\n",
    "#     for j in range(num_features):\n",
    "#         gradient = 0\n",
    "#         for i in range(m):\n",
    "#             # Calculate the gradient for each feature\n",
    "#             z = np.dot(theta, X[i])\n",
    "#             # print(z)\n",
    "#             h = Sigmoid(z)\n",
    "#             # print(h)\n",
    "#             gradient += (h - Y[i]) * X[i][j]\n",
    "#         # print(gradient)\n",
    "        \n",
    "#         # Update the j-th weight in theta\n",
    "#         # print(alpha * (1 / m) * gradient)\n",
    "#         new_theta[j] = new_theta[j] - alpha * (1 / m) * gradient\n",
    "    return new_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 逻辑斯蒂回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Logistic_Regression(X,Y,alpha,theta,num_iters):\n",
    "    \"\"\"\n",
    "    多轮迭代梯度下降\n",
    "    输入：X（数据）、Y（标签）、theta(初始化的theta)、alpha（学习率）、num_iters（迭代梯度下降的轮数）\n",
    "    输出：theta（最终学习得到的theta）、epoch_loss（各轮cost值（有间隔的取样）组成的list,用于绘制cost曲线图）\n",
    "    \"\"\"\n",
    "    m = len(Y)\n",
    "    epoch_loss = []\n",
    "    i = 0\n",
    "    while i < num_iters:\n",
    "        new_theta = Gradient_Descent(X,Y,theta,alpha)\n",
    "        theta = new_theta\n",
    "        # print(theta)\n",
    "        i = i + 1\n",
    "        if i % 50 == 0:\n",
    "            cost = Cost_Function(X, Y, theta)\n",
    "            epoch_loss.append(cost)\n",
    "    return theta, epoch_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#超参数设置与初始化\n",
    "initial_theta = [1,1] #学习参数初始化\n",
    "alpha = 0.01 #学习率\n",
    "iterations = 1000 #训练轮数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#opt_theta最终学习到的模型参数theta\n",
    "opt_theta, epoch_loss = Logistic_Regression(X_train,Y_train,alpha,initial_theta,iterations)\n",
    "# print(epoch_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型评估与绘制cost曲线 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cost_plot(epoch_losses, iterations, figure_name, imgPath = \"./\"):\n",
    "    plt.switch_backend('Agg') \n",
    "    plt.figure() \n",
    "    x = list(range(0,iterations,50))\n",
    "    plt.plot(x, epoch_losses,'b',label = 'loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend('Logistic Regression')        \n",
    "    plt.savefig(os.path.join(imgPath,figure_name)) \n",
    "    \n",
    "figure_name = \"1_recon_loss.jpg\"    \n",
    "cost_plot(epoch_loss,iterations,figure_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.8181818181818182\n"
     ]
    }
   ],
   "source": [
    "def evaluation(X_test,Y_test,theta):\n",
    "    length = len(X_test)\n",
    "    score = 0\n",
    "    for i in range(length):\n",
    "        prediction = round(Hypothesis(X_test[i],theta))#用round函数四舍五入，将概率映射到标签\n",
    "        answer = Y_test[i]\n",
    "        if prediction == answer:\n",
    "            score += 1\n",
    "    score = float(score) / float(length)\n",
    "    print('Accuracy = ',score)\n",
    "\n",
    "evaluation(X_test,Y_test,opt_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请尝试不同的初始化、学习率、训练轮数来训练模型，并绘制cost曲线，观察准确率"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
